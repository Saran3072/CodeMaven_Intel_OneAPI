{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11c94926-aae6-4427-bd6a-f52c57a7138d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glob/development-tools/versions/oneapi/2022.3.1/oneapi/intelpython/latest/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from daal4py.sklearn.ensemble import RandomForestRegressor\n",
    "from daal4py.sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f6c59ae-37ae-45df-ae4c-acb0ac446e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train and test datasets using Pandas\n",
    "train_data = pd.read_csv(\"openAPITrain.csv\")\n",
    "test_data = pd.read_csv(\"openAPITest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc7c3d14-c941-4af0-a690-c4841e1fdc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values (if any)\n",
    "train_data.fillna('', inplace=True)\n",
    "test_data.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39f0085e-e450-4936-b567-e2377f3eae11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IDLink         0\n",
       "Title          0\n",
       "Headline       0\n",
       "Source         0\n",
       "Topic          0\n",
       "PublishDate    0\n",
       "Facebook       0\n",
       "GooglePlus     0\n",
       "LinkedIn       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values in training data\n",
    "test_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0fe543e-cfdb-49c8-a3fb-6b3418789122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LabelEncoder object\n",
    "le = LabelEncoder()\n",
    "# Encode the categorical features in training data\n",
    "train_data['Topic'] = le.fit_transform(train_data['Topic'])\n",
    "train_data['Source'] = le.fit_transform(train_data['Source'])\n",
    "# Encode the categorical features in testing data\n",
    "test_data['Topic'] = le.fit_transform(test_data['Topic'])\n",
    "test_data['Source'] = le.fit_transform(test_data['Source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52abf0ba-8705-4d48-9ef8-eedade19d6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the timestamp to a suitable format\n",
    "train_data['PublishDate'] = pd.to_datetime(train_data['PublishDate'])\n",
    "test_data['PublishDate'] = pd.to_datetime(test_data['PublishDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fee230f9-df30-41a7-bb97-d28d73bc7dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/u190070/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/u190070/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/u190070/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Convert to lowercase and remove punctuation\n",
    "    words = [token.lower() for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    # Remove stopwords\n",
    "    words_filtered = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Lemmatize the words\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words_filtered]\n",
    "\n",
    "    return ' '.join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1b2af58-ab86-4634-80e7-4bae6621331b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text data (Title and Headline columns)\n",
    "train_data['Title'] = train_data['Title'].apply(preprocess_text)\n",
    "train_data['Headline'] = train_data['Headline'].apply(preprocess_text)\n",
    "test_data['Title'] = test_data['Title'].apply(preprocess_text)\n",
    "test_data['Headline'] = test_data['Headline'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "570f7b1a-f1b4-4d0e-8dbb-6f0d3ec6d1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "title_vectorizer = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "headline_vectorizer = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "\n",
    "def create_features(df, fit_vectorizers = False):\n",
    "    # Word count for Title and Headline\n",
    "    df['Title_word_count'] = df['Title'].apply(lambda x: len(x.split()))\n",
    "    df['Headline_word_count'] = df['Headline'].apply(lambda x: len(x.split()))\n",
    "\n",
    "    # Sentiment polarity and subjectivity for Title and Headline\n",
    "    df['Title_polarity'] = df['Title'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    df['Title_subjectivity'] = df['Title'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "    df['Headline_polarity'] = df['Headline'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    df['Headline_subjectivity'] = df['Headline'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "\n",
    "    global title_vectorizer, headline_vectorizer\n",
    "    if fit_vectorizers:\n",
    "        title_vectorizer.fit(df['Title'])\n",
    "        headline_vectorizer.fit(df['Headline'])\n",
    "    \n",
    "    title_tfidf = title_vectorizer.transform(df['Title']).toarray()\n",
    "    headline_tfidf = headline_vectorizer.transform(df['Headline']).toarray()\n",
    "\n",
    "    # Combine the existing features with the new TF-IDF features\n",
    "    title_tfidf_df = pd.DataFrame(title_tfidf, columns=[f'Title_tfidf_{i}' for i in range(title_tfidf.shape[1])], index=df.index)\n",
    "    headline_tfidf_df = pd.DataFrame(headline_tfidf, columns=[f'Headline_tfidf_{i}' for i in range(headline_tfidf.shape[1])], index=df.index)\n",
    "\n",
    "    df = pd.concat([df, title_tfidf_df, headline_tfidf_df], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fd99ceb-7b66-40c6-a3bd-e3622ca683c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features from text data\n",
    "train_data = create_features(train_data, fit_vectorizers=True)\n",
    "test_data = create_features(test_data, fit_vectorizers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36d15735-5cc3-42c4-bc92-0f80394e844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the numerical features\n",
    "scaler = MinMaxScaler()\n",
    "train_data[['Facebook', 'GooglePlus', 'LinkedIn']] = scaler.fit_transform(train_data[['Facebook', 'GooglePlus', 'LinkedIn']])\n",
    "test_data[['Facebook', 'GooglePlus', 'LinkedIn']] = scaler.transform(test_data[['Facebook', 'GooglePlus', 'LinkedIn']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "941e112c-3aca-4cae-9442-57364ffa0d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train_data.drop(['IDLink', 'PublishDate', 'Title', 'Headline', 'SentimentTitle', 'SentimentHeadline'], axis=1)\n",
    "y_test = test_data.drop(['IDLink', 'PublishDate', 'Title', 'Headline'], axis=1)\n",
    "y_title = train_data['SentimentTitle']\n",
    "y_headline = train_data['SentimentHeadline']\n",
    "\n",
    "X_train, X_test, y_title_train, y_title_test, y_headline_train, y_headline_test = train_test_split(X, y_title, y_headline, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79cf086c-126e-469d-99c0-ca06d43d0f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Facebook</th>\n",
       "      <th>GooglePlus</th>\n",
       "      <th>LinkedIn</th>\n",
       "      <th>Title_word_count</th>\n",
       "      <th>Headline_word_count</th>\n",
       "      <th>Title_polarity</th>\n",
       "      <th>Title_subjectivity</th>\n",
       "      <th>Headline_polarity</th>\n",
       "      <th>...</th>\n",
       "      <th>Headline_tfidf_90</th>\n",
       "      <th>Headline_tfidf_91</th>\n",
       "      <th>Headline_tfidf_92</th>\n",
       "      <th>Headline_tfidf_93</th>\n",
       "      <th>Headline_tfidf_94</th>\n",
       "      <th>Headline_tfidf_95</th>\n",
       "      <th>Headline_tfidf_96</th>\n",
       "      <th>Headline_tfidf_97</th>\n",
       "      <th>Headline_tfidf_98</th>\n",
       "      <th>Headline_tfidf_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3560</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>376</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>376</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2610</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3286</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40503</th>\n",
       "      <td>1469</td>\n",
       "      <td>2</td>\n",
       "      <td>0.016627</td>\n",
       "      <td>0.060726</td>\n",
       "      <td>0.005201</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40504</th>\n",
       "      <td>3398</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40505</th>\n",
       "      <td>2564</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.012618</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.431484</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40506</th>\n",
       "      <td>546</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001469</td>\n",
       "      <td>0.001577</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40507</th>\n",
       "      <td>1469</td>\n",
       "      <td>0</td>\n",
       "      <td>0.016431</td>\n",
       "      <td>0.060726</td>\n",
       "      <td>0.005201</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40508 rows Ã— 211 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Source  Topic  Facebook  GooglePlus  LinkedIn  Title_word_count  \\\n",
       "0        3560      2  0.000000    0.000000  0.000000                 6   \n",
       "1         376      0  0.000000    0.000000  0.000000                 4   \n",
       "2         376      0  0.000000    0.000000  0.000000                 6   \n",
       "3        2610      0  0.000000    0.000000  0.000000                 4   \n",
       "4        3286      0  0.000000    0.000000  0.000000                 7   \n",
       "...       ...    ...       ...         ...       ...               ...   \n",
       "40503    1469      2  0.016627    0.060726  0.005201                 9   \n",
       "40504    3398      3  0.000245    0.000789  0.000274                 8   \n",
       "40505    2564      3  0.000539    0.012618  0.000274                 7   \n",
       "40506     546      2  0.001469    0.001577  0.000274                 7   \n",
       "40507    1469      0  0.016431    0.060726  0.005201                 9   \n",
       "\n",
       "       Headline_word_count  Title_polarity  Title_subjectivity  \\\n",
       "0                       14        0.000000            0.000000   \n",
       "1                       15        0.000000            0.000000   \n",
       "2                       14        0.000000            0.000000   \n",
       "3                       18        0.000000            0.000000   \n",
       "4                       16        0.000000            0.000000   \n",
       "...                    ...             ...                 ...   \n",
       "40503                   16        0.100000            0.100000   \n",
       "40504                   17        0.400000            0.800000   \n",
       "40505                   13        0.000000            0.000000   \n",
       "40506                   19        0.136364            0.454545   \n",
       "40507                   16        0.100000            0.100000   \n",
       "\n",
       "       Headline_polarity  ...  Headline_tfidf_90  Headline_tfidf_91  \\\n",
       "0              -0.100000  ...           0.000000                0.0   \n",
       "1               0.100000  ...           0.000000                0.0   \n",
       "2               0.000000  ...           0.000000                0.0   \n",
       "3              -0.166667  ...           0.000000                0.0   \n",
       "4               0.133333  ...           0.000000                0.0   \n",
       "...                  ...  ...                ...                ...   \n",
       "40503           0.000000  ...           0.000000                0.0   \n",
       "40504           0.200000  ...           0.000000                0.0   \n",
       "40505           0.000000  ...           0.431484                0.0   \n",
       "40506           0.500000  ...           0.000000                0.0   \n",
       "40507           0.100000  ...           0.000000                0.0   \n",
       "\n",
       "       Headline_tfidf_92  Headline_tfidf_93  Headline_tfidf_94  \\\n",
       "0                    0.0                0.0                0.0   \n",
       "1                    0.0                0.0                0.0   \n",
       "2                    0.0                0.0                0.0   \n",
       "3                    0.0                0.0                0.0   \n",
       "4                    0.0                0.0                0.0   \n",
       "...                  ...                ...                ...   \n",
       "40503                0.0                0.0                0.0   \n",
       "40504                0.0                0.0                0.0   \n",
       "40505                0.0                0.0                0.0   \n",
       "40506                0.0                0.0                0.0   \n",
       "40507                0.0                0.0                0.0   \n",
       "\n",
       "       Headline_tfidf_95  Headline_tfidf_96  Headline_tfidf_97  \\\n",
       "0                    0.0                0.0                0.0   \n",
       "1                    0.0                0.0                0.0   \n",
       "2                    0.0                0.0                0.0   \n",
       "3                    0.0                0.0                0.0   \n",
       "4                    0.0                0.0                0.0   \n",
       "...                  ...                ...                ...   \n",
       "40503                0.0                0.0                0.0   \n",
       "40504                0.0                0.0                0.0   \n",
       "40505                0.0                0.0                0.0   \n",
       "40506                0.0                0.0                0.0   \n",
       "40507                0.0                0.0                0.0   \n",
       "\n",
       "       Headline_tfidf_98  Headline_tfidf_99  \n",
       "0                    0.0                0.0  \n",
       "1                    0.0                0.0  \n",
       "2                    0.0                0.0  \n",
       "3                    0.0                0.0  \n",
       "4                    0.0                0.0  \n",
       "...                  ...                ...  \n",
       "40503                0.0                0.0  \n",
       "40504                0.0                0.0  \n",
       "40505                0.0                0.0  \n",
       "40506                0.0                0.0  \n",
       "40507                0.0                0.0  \n",
       "\n",
       "[40508 rows x 211 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "809b9b77-5ca9-4d5c-ae75-ef295d7c22f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Ridge Regression and Random Forest Regressor models\n",
    "ridge_title = Ridge().fit(X_train, y_title_train)\n",
    "ridge_headline = Ridge().fit(X_train, y_headline_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5315d07-fd6b-40fc-b257-6b360fba5400",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_title = RandomForestRegressor(n_estimators=500, max_depth=5, random_state=42).fit(X_train, y_title_train)\n",
    "rf_headline = RandomForestRegressor(n_estimators=500, max_depth=5, random_state=42).fit(X_train, y_headline_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdcd5183-87d7-442d-9bca-31778246112e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base estimators for stacking\n",
    "base_estimators = [\n",
    "    ('ridge_title', Ridge(alpha = 10.0)),\n",
    "    ('ridge_headline', Ridge(alpha = 10.0)),\n",
    "    ('rf_title', RandomForestRegressor(n_estimators=500, max_depth=5, random_state=42)),\n",
    "    ('rf_headline', RandomForestRegressor(n_estimators=500, max_depth=5, random_state=42))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "851116f0-3d07-48ca-9de7-66b8c5a2b880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Stacking Regressor for both SentimentTitle and SentimentHeadline\n",
    "stacking_title = StackingRegressor(estimators=base_estimators, final_estimator=Ridge())\n",
    "stacking_headline = StackingRegressor(estimators=base_estimators, final_estimator=Ridge())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1ba8b0-de46-413d-8395-4f1373010c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the stacking models\n",
    "stacking_title.fit(X_train, y_title_train)\n",
    "stacking_headline.fit(X_train, y_headline_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a4cc6d-f8c9-4ca2-9afe-871556bef57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "# Evaluate the models' performance using Stacking Regressor\n",
    "stacking_title_pred = stacking_title.predict(X_test)\n",
    "stacking_headline_pred = stacking_headline.predict(X_test)\n",
    "\n",
    "mse_stacking_title = mean_squared_error(y_title_test, stacking_title_pred)\n",
    "mse_stacking_headline = mean_squared_error(y_headline_test, stacking_headline_pred)\n",
    "\n",
    "print(f'MSE Stacking Title: {mse_stacking_title}')\n",
    "print(f'MSE Stacking Headline: {mse_stacking_headline}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6337923-8013-4911-a5d1-3e6ed34e3189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained and optimized models to predict SentimentTitle and SentimentHeadline for the test dataset\n",
    "test_data['SentimentTitle'] = stacking_title.predict(y_test)\n",
    "test_data['SentimentHeadline'] = stacking_headline.predict(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc8776d-99dc-45a6-9fec-517a6b090513",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[['IDLink', 'SentimentTitle', 'SentimentHeadline']].to_csv(\"output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df4dace-d954-4183-91e0-0427bfc174eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[['IDLink', 'SentimentTitle', 'SentimentHeadline']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1683de50-cc86-4d39-b54c-68885f16683a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oneAPI 2022.3.1",
   "language": "python",
   "name": "2022.3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
